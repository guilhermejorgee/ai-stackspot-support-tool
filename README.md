# Stackspot LiteLLM Proxy

Proxy LiteLLM altamente otimizado para integra√ß√£o com Stackspot AI, com suporte completo a streaming, tool calling e logging estrat√©gico.

## üöÄ Caracter√≠sticas Principais

### ‚úÖ Implementa√ß√£o Completa
- **4 m√©todos LiteLLM**: `completion`, `acompletion`, `streaming`, `astreaming`
- **Tool Calling**: Suporte total a ferramentas OpenAI-compat√≠veis
- **Streaming SSE**: Resposta em tempo real com Server-Sent Events
- **Correlation ID**: Suporte obrigat√≥rio para rastreamento de conversas

### üèóÔ∏è Arquitetura Refatorada
- **~200 linhas eliminadas** atrav√©s de m√©todos compartilhados
- **Anti-over-engineering**: Apenas abstra√ß√µes necess√°rias
- **M√©todos p√∫blicos minimalistas**: 1-3 linhas cada
- **C√≥digo DRY**: Zero duplica√ß√£o entre sync/async

### üìä Logging Estrat√©gico
- **Autentica√ß√£o**: Sucesso/falha com detalhes
- **Tool Calls**: Detec√ß√£o e argumentos
- **SSE Connections**: Monitoramento de conex√µes
- **Performance**: M√©tricas de chunks processados

## üìã Pr√©-requisitos

### Depend√™ncias
```bash
pip install -r requirements.txt
```

### Vari√°veis de Ambiente
Crie um arquivo `.env`:
```bash
CLIENT_ID=seu_client_id_stackspot
CLIENT_SECRET=seu_client_secret_stackspot
REALM=seu_realm
GENAI_AGENT_ID=seu_agent_id
```

## üîß Configura√ß√£o

### 1. Configurar config.yaml
```yaml
model_list:
  - model_name: stackspot-chat
    litellm_params:
      model: stackspot-chat
      custom_llm_provider: stackspot
```

### 2. Iniciar o Proxy
```bash
litellm --config config.yaml --port 4000
```

## üíª Uso com PydanticAI

### Configura√ß√£o B√°sica
```python
from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIModel

# Configurar modelo
model = OpenAIModel(
    model_name='stackspot-chat',
    base_url='http://localhost:4000/v1',
    api_key='fake-key'  # N√£o √© necess√°ria
)

# Criar agente com tools
agent = Agent(
    model=model,
    tools=[sua_funcao_tool],
    system_prompt="Voc√™ √© um assistente √∫til..."
)
```

### Com Correlation ID (Obrigat√≥rio)
```python
import httpx

# Cliente customizado com correlation ID
client = httpx.AsyncClient(
    headers={'correlation-id': 'sua-conversa-123'}
)

model = OpenAIModel(
    model_name='stackspot-chat',
    base_url='http://localhost:4000/v1',
    api_key='fake-key',
    http_client=client
)
```

### Executar com Streaming
```python
# Streaming autom√°tico com PydanticAI
result = await agent.run("Sua pergunta aqui", stream=True)
print(result.data)
```

## üõ†Ô∏è Features Avan√ßadas

### Tool Calling
O proxy detecta automaticamente tool calls do Stackspot:
```
FUNCTION_CALL_START
function_name
arguments: {"param": "value"}
FUNCTION_CALL_END
```

### Logging Configur√°vel
```python
import logging

# Debug detalhado
logging.getLogger('custom_handler').setLevel(logging.DEBUG)

# Produ√ß√£o (apenas erros)
logging.getLogger('custom_handler').setLevel(logging.ERROR)
```

### M√©tricas de Performance
- Tool calls detectados por requisi√ß√£o
- Chunks de streaming processados  
- Tamanho das respostas coletadas
- Tempo de autentica√ß√£o JWT

## üîç Troubleshooting

### Erro: "correlation_id ausente"
```python
# ‚úÖ Correto - incluir header
headers = {'correlation-id': 'conversa-123'}

# ‚ùå Incorreto - sem header
# Resultar√° em KeyError
```

### Autentica√ß√£o Falhando
```bash
# Verificar logs
tail -f litellm.log | grep "Authentication"

# Testar manualmente
python -c "from custom_handler import StackspotLLM; s=StackspotLLM(); s.authenticate()"
```

### Streaming N√£o Funciona
```python
# Verificar se est√° usando stream=True
result = await agent.run("pergunta", stream=True)

# Debug de conex√£o SSE
logging.getLogger('custom_handler').setLevel(logging.DEBUG)
```

### Performance Lenta
- **Rate Limiting**: Stackspot pode ter limites
- **JWT Caching**: Tokens s√£o reutilizados automaticamente  
- **Connection Pooling**: Use `httpx.AsyncClient` persistente

## üìä Monitoramento

### Logs Estrat√©gicos
```bash
# Iniciar com logs detalhados
export PYTHONPATH=.
litellm --config config.yaml --port 4000 --debug

# Filtrar apenas erros cr√≠ticos
litellm --config config.yaml --port 4000 2>&1 | grep ERROR
```

### M√©tricas em Tempo Real
- `INFO`: Tool calls encontrados
- `DEBUG`: Chunks de streaming processados
- `ERROR`: Falhas de autentica√ß√£o/conex√£o
- `WARNING`: Parsing de eventos falhou

## üèõÔ∏è Arquitetura Interna

### M√©todos P√∫blicos (Ultra-Simples)
```python
def completion(self, model, messages, **kwargs):
    url, headers, payload = self._prepare_streaming_request(messages, **kwargs)
    return self._collect_completion_response(url, headers, payload, model)

def streaming(self, model, messages, **kwargs):
    url, headers, payload = self._prepare_streaming_request(messages, **kwargs)
    yield from self._process_streaming_events(url, headers, payload)
```

### M√©todos Compartilhados
- `_prepare_streaming_request()`: Setup comum
- `_collect_completion_response()`: Coleta SSE para completion
- `_process_streaming_events()`: Streaming em tempo real
- `_process_tool_calls_streaming()`: Tool calls progressivos

## üö¶ API Endpoints

Quando o proxy estiver rodando:

- **Chat Completions**: `POST http://localhost:4000/v1/chat/completions`
- **Streaming**: Mesmo endpoint com `"stream": true`
- **Models**: `GET http://localhost:4000/v1/models`
- **Health**: `GET http://localhost:4000/health`

## üìù Exemplo Completo

```python
import asyncio
from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIModel
import httpx

async def main():
    # Cliente com correlation ID
    client = httpx.AsyncClient(
        headers={'correlation-id': 'conversa-teste-123'}
    )
    
    # Modelo configurado
    model = OpenAIModel(
        model_name='stackspot-chat',
        base_url='http://localhost:4000/v1', 
        api_key='fake-key',
        http_client=client
    )
    
    # Agente com tool
    def buscar_tempo(cidade: str) -> str:
        return f"Tempo em {cidade}: 25¬∞C, ensolarado"
    
    agent = Agent(
        model=model,
        tools=[buscar_tempo],
        system_prompt="Use as ferramentas dispon√≠veis para responder."
    )
    
    # Executar com streaming
    result = await agent.run(
        "Qual o tempo em S√£o Paulo?",
        stream=True
    )
    
    print(f"Resposta: {result.data}")
    print(f"Tool calls: {len([m for m in result.all_messages() if m.role == 'tool'])}")

if __name__ == "__main__":
    asyncio.run(main())
```

## üéØ Pr√≥ximos Passos

1. **Produ√ß√£o**: HTTPS, autentica√ß√£o, rate limiting
2. **Observabilidade**: Prometheus metrics, OpenTelemetry  
3. **Scaling**: Load balancer, m√∫ltiplas inst√¢ncias
4. **Cache**: Redis para respostas frequentes
5. **Monitoring**: Health checks, alertas autom√°ticos